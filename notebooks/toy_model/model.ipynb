{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "# For N-dimensional array manipulation\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "# For data analysis and data structures in DataFrames\n",
    "import pandas as pd\n",
    "# For data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# For machine learning algorithms and evaluation metrics\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "#import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "# import TextVectorization from keras\n",
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('media_bias.csv')\n",
    "\n",
    "# Clean dataset\n",
    "df = df[df.Label_bias != 'No agreement']\n",
    "df = df[df.article != 'NaN']\n",
    "df = df[df.sentence != 'NaN']\n",
    "\n",
    "# Replace label with 0, 1\n",
    "df['Label_bias'] = df['Label_bias'].replace('Biased', 0)\n",
    "df['Label_bias'] = df['Label_bias'].replace('Non-biased', 1)\n",
    "\n",
    "# Only use sentence column and bias column\n",
    "df = df[['sentence', 'Label_bias']]\n",
    "df = df.rename(columns={'sentence': 'text', 'Label_bias': 'label'})\n",
    "\n",
    "# Split data into X and y\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Remove numbers from all strings in X\n",
    "X = X.str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# Remove punctuation from all strings in X\n",
    "X = X.str.replace('[^\\w\\s]','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (992,)\n",
      "X_test shape:  (311,)\n",
      "X_train shape:  (992,)\n",
      "X_test shape:  (311,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, validation and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shape of train, validation and test\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "# print(\"X_val shape: \", X_val.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "\n",
    "\n",
    "# Flatten X_train for training and X_test for testing\n",
    "X_train = np.array(X_train).flatten()\n",
    "X_test = np.array(X_test).flatten()\n",
    "    \n",
    "# X_train = X_train.to_numpy()\n",
    "# X_train = np.array(X_train).flatten()\n",
    "# print x shape\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "# print test shape\n",
    "print(\"X_test shape: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_2 (TextV  (None, 100)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,387,137\n",
      "Trainable params: 1,387,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n",
      "16/16 [==============================] - 11s 287ms/step - loss: 0.6761 - accuracy: 0.6321 - val_loss: 0.6361 - val_accuracy: 0.6653\n",
      "Epoch 2/4\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 0.6198 - accuracy: 0.6502 - val_loss: 0.6097 - val_accuracy: 0.6653\n",
      "Epoch 3/4\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 0.3950 - accuracy: 0.8044 - val_loss: 0.9314 - val_accuracy: 0.6653\n",
      "Epoch 4/4\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.1021 - accuracy: 0.9718 - val_loss: 0.9492 - val_accuracy: 0.7016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DNN using tensorflow\n",
    "vectorize_layer = TextVectorization(max_tokens=10000, output_mode='int', output_sequence_length=100)\n",
    "vectorize_layer.adapt(X_train)\n",
    "model = keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(input_dim=10000, output_dim=128, mask_zero=True),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=4, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 18ms/step - loss: 1.0165 - accuracy: 0.6399\n",
      "Loss:  1.016514778137207\n",
      "Accuracy:  0.63987135887146\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[[0.01849239]\n",
      " [0.9917003 ]] 0 is bias, 1 is non-bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model.save('model/saved_model')\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "prediction = model.predict([\"YouTube is making clear there will be no “birtherism” on its platform during this year’s U.S. presidential election – a belated response to a type of conspiracy theory more prevalent in the 2012 race.\", \"The increasingly bitter dispute between American women’s national soccer team and the U.S. Soccer Federation spilled onto the field Wednesday night when players wore their warm-up jerseys inside outin a protest before their 3-1 victory over Japan.\"])\n",
    "print(prediction, \"0 is bias, 1 is non-bias\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\verni\\Desktop\\AI course\\group-07\\notebooks\\toy_model/model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\verni\\Desktop\\AI course\\group-07\\notebooks\\toy_model/model/1/assets\n"
     ]
    }
   ],
   "source": [
    "parent_dir = os.path.split(os.getcwd())[0] + \"\\\\\" + os.path.split(os.getcwd())[1]\n",
    "save_path = parent_dir + \"/model/1/\"\n",
    "# tf.saved_model.save(model, save_path) - DOESN'T SAVE THE LAYERS\n",
    "\n",
    "model.save(save_path, save_format='tf') # ERROR states layers aren't saved, but keras_metadata.pb is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def _serving_input_receiver_fn():\n",
    "#     serialized_tf_example = tf.placeholder(dtype=tf.string, shape=None, \n",
    "#                                            name='input_example_tensor')\n",
    "#     # key (e.g. 'examples') should be same with the inputKey when you \n",
    "#     # buid the request for prediction\n",
    "#     receiver_tensors = {'examples': serialized_tf_example}\n",
    "#     inputs = {'text': tf.placeholder(tf.string, [None])}\n",
    "#     return tf.estimator.export.ServingInputReceiver(inputs, receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets:\n",
      "example_bucket_v1\n",
      "example_bucket_v2-aiproject-dit825\n",
      "Listed all storage buckets.\n",
      "Models:\n",
      "Listed all models.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "import os\n",
    "project_id = 'dit825'\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
    "storage_client = storage.Client(project=project_id)\n",
    "buckets = storage_client.list_buckets()\n",
    "print(\"Buckets:\")\n",
    "for bucket in buckets:\n",
    "    print(bucket.name) \n",
    "print(\"Listed all storage buckets.\")\n",
    "# List all models in the project from aiplatform\n",
    "aiplatform.init(project=project_id, location='europe-west4')\n",
    "models = aiplatform.Model.list()\n",
    "print(\"Models:\")\n",
    "for model in models:\n",
    "    print(model)\n",
    "print(\"Listed all models.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2abd1138ce1c26e3850e921765871411c611b4978bcd07656c7e4d1d85e9831"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
