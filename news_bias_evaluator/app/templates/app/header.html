<header class="header-container">
  {% load static %}
  <h1>
    <img class="logo" src="{% static 'app/images/logo.svg' %}" alt="logo" />
    Evaluator
  </h1>
  <div class="container-btn">
    {% if user.is_authenticated %}
    <a title="Log out" href="{% url 'app:logout' %}"
      ><button class="logout-btn nav-btn"></button
    ></a>
    {% else %}
    <a title="Log in" href="{% url 'app:login' %}"
      ><button class="login-btn nav-btn"></button
    ></a>
    {% endif %}
    <button title="Information" class="info-btn nav-btn" onclick="showDialog()">
      <div style="display: none">
        Icon made from
        <a href="http://www.onlinewebfonts.com/icon">Icon Fonts</a> is licensed
        by CC BY 3.0
      </div>
    </button>
    <a title="GitHub repository" href="https://github.com/gusaxtcha/DIT825"
      ><button class="github-btn nav-btn"></button
    ></a>
  </div>
</header>
<div>
    {% if messages %}
        <ul class="messages">
        {% for message in messages %}
        <li>{{ message }}</li>
        {% endfor %}
        </ul>
    {% endif %}
</div>
<dialog id="infoDialog" class="info-dialog">
  <header>
    <span onclick="hideDialog()" class="close-button topright">&times;</span>
  </header>
  <h3>Model info</h3>
  <p>
    Text-based classification models set complex problems for Machine Learning
    through the nature of context involved in sentences. In ML models
    responsible for translating text, the context of previous words in the
    sentence influences the following word. The model architecture for the
    <b>Bias Evaluator</b> is based on the Bidirectional Encoders from
    Transformers (BERT) model. The model is available on Tensorflow Hub, which
    hosts a
    <a
      href="https://tfhub.dev/google/collections/bert/1"
      style="color: rgb(161, 242, 246)"
      >collection</a
    >
    of models openly available for training. The model we've selected and use in
    this project can be found
    <a
      href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1"
      style="color: rgb(161, 242, 246)"
    >
      here</a
    >.
  </p>
  <h3>Dataset info</h3>
  <p>
    The dataset used to train our model was found on Kaggle and can be accessed
    <a
      href="https://www.kaggle.com/datasets/timospinde/mbic-a-media-bias-annotation-dataset"
      style="color: rgb(161, 242, 246)"
      >here</a
    >.<br />
    The dataset consists of 1700 elements, 1550 of which are useable for our
    application. Typically to achieve high accuracy, the model would need to be
    trained with a relatively large dataset, however, to overcome the small
    dataset, we have used learning methods to increase the size of our dataset,
    explained below.
  </p>
  <h3>Model training</h3>
  <p>
    Initial training is done using the above mentioned dataset. <br />Due to the
    size of the dataset we decided to use semi-supervised learning to increase
    the size of the labelled data available to us. Once the initial model has
    been trained, we feed it unlabelled data we've collected, using the results
    from the model that data is then labelled. Once we have enough of this newly
    labelled data, we re-train the model. If the re-train results in a higher
    model accuracy, we use this as the final model.
  </p>
</dialog>

<script>
  function showDialog() {
    infoDialog.showModal();
  }
  function hideDialog() {
    document.getElementById("infoDialog").close();
  }
</script>
